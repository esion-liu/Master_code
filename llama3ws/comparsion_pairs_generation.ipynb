{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "738d90fa-edc0-40a2-8c58-f6636492eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this note book is used for pairs comparsion for users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ebc82-8c79-4a63-bebd-17124a040dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "postgres_conn_params = {\n",
    "    'host' : '192.168.0.148',\n",
    "    'port' : '5432',\n",
    "    'user' : 'postgres',\n",
    "    'password' : 'hide',\n",
    "    'database' : 'steam'\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**postgres_conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d32e3b-0f97-48a0-9004-4d9b6af069cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only select users who had made 10+ reviews totally\n",
    "SQL = \"\"\"\n",
    "    SELECT DISTINCT reviewer_name\n",
    "    FROM (SELECT reviewer_name\n",
    "        FROM game_reviews\n",
    "        WHERE length(review_text) >= 20\n",
    "        GROUP BY reviewer_name\n",
    "        HAVING count(game_name) > 10) as temp\n",
    "\"\"\"\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(SQL)\n",
    "users = cursor.fetchall()\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2779baec-9063-4882-ba1c-04456218ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147402\n",
      "['https://steamcommunity.com/profiles/76561197971248094/', 'https://steamcommunity.com/profiles/76561198038954363/', 'https://steamcommunity.com/id/BoneHeadBrianTV/', 'https://steamcommunity.com/profiles/76561198006265186/', 'https://steamcommunity.com/profiles/76561198069439251/']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "users = [u for (u,) in users]\n",
    "random.shuffle(users)\n",
    "print(len(users))\n",
    "print(users[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87b23020-76cb-4e92-b615-eed081138b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load, of the shuffled lists since may not able to process all data at a time.\n",
    "import pickle\n",
    "with open(\"data/user_list.pickle\", \"wb\") as f:\n",
    "    pickle.dump(users, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ea6b7-2286-4698-bfcc-aa3f186f448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start from here if rerunning the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e51b2bf-6da6-48ca-a957-51bd517ce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/user_list.pickle\", \"rb\") as f:\n",
    "    users = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5bf2837-f5d0-4e02-8744-e7db8132bbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://steamcommunity.com/profiles/76561197971248094/', 'https://steamcommunity.com/profiles/76561198038954363/', 'https://steamcommunity.com/id/BoneHeadBrianTV/', 'https://steamcommunity.com/profiles/76561198006265186/', 'https://steamcommunity.com/profiles/76561198069439251/']\n",
      "147402\n"
     ]
    }
   ],
   "source": [
    "print(users[:5])\n",
    "print(len(users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6f5eb98-ebf3-4214-ac92-dc717245782e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "# load language model\n",
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ll3_fitting/epoch_329\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35544725-bc65-43bd-92cb-262e60bb6afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = '<|reserved_special_token_250|>'\n",
    "tokenizer.pad_token_id = 128255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6850e484-c0c4-4ea7-8849-dbce6a266906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generation(inputs):\n",
    "    outputs = []\n",
    "    for i in range(len(inputs)):\n",
    "        temp = []\n",
    "        temp.append({'role': 'system', 'content': \"You are a personal judge of video game, your role is to judge which game is preferred by the user based on user's feedbacks of two games. Simply reply with prefered game's name, no need for explanation\"})\n",
    "        temp.append({\n",
    "            'role': 'user',\n",
    "            'content': f\"I played two games {inputs[i][0][0]}, and {inputs[i][1][0]}. After playing, I gave reviews for both games as follow:\\n{inputs[i][0][0]}: {inputs[i][0][1]}\\n{inputs[i][1][0]}: {inputs[i][1][1]}\"\n",
    "        })\n",
    "        outputs.append(temp)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8b3f2b4-055d-40ff-8c58-b60d1f95ec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_title(title: str):\n",
    "    title = title.replace('.', '')\n",
    "    title = title.replace(',', '')\n",
    "    title = title.replace('/', '')\n",
    "    title = title.replace(' ', '')\n",
    "    title = title.replace(\"'\", '')\n",
    "    title = title.replace(\"-\", '')\n",
    "    # Remove non-ascii characters\n",
    "    title = title.encode(\"ascii\", errors=\"ignore\").decode()\n",
    "    # Remove extra spaces\n",
    "    title = re.sub(' +', ' ', title)\n",
    "    # Convert to lowercase\n",
    "    title = title.lower()\n",
    "    # Strip leading and trailing spaces\n",
    "    title = title.strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1140ed-7814-4353-bf30-a7e84e37614d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "postgres_conn_params = {\n",
    "    'host' : '192.168.0.148',\n",
    "    'port' : '5432',\n",
    "    'user' : 'postgres',\n",
    "    'password' : 'hide',\n",
    "    'database' : 'steam'\n",
    "}\n",
    "\n",
    "conn = psycopg2.connect(**postgres_conn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36092e93-6fc5-49a3-9046-01591b3df71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = users.index('https://steamcommunity.com/id/vggandros/')\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfb49eca-5f74-4803-a827-654fd7e2c8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 9775 users, 9775th user is https://steamcommunity.com/id/SwampLord420/\n",
      "processed 9800 users, 9800th user is https://steamcommunity.com/id/lil_hj/\n",
      "processed 9825 users, 9825th user is https://steamcommunity.com/id/ViolenceFaith/\n",
      "processed 9850 users, 9850th user is https://steamcommunity.com/profiles/76561198249595398/\n",
      "processed 9875 users, 9875th user is https://steamcommunity.com/id/iSshkin/\n",
      "processed 9900 users, 9900th user is https://steamcommunity.com/id/GrandeLucao/\n",
      "processed 9925 users, 9925th user is https://steamcommunity.com/profiles/76561198079437668/\n",
      "processed 9950 users, 9950th user is https://steamcommunity.com/id/latsyrcmoy/\n",
      "processed 9975 users, 9975th user is https://steamcommunity.com/id/braprancher/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "conn.rollback()\n",
    "queryer = conn.cursor()\n",
    "saver = conn.cursor()\n",
    "\n",
    "Insert_SQL = \"\"\"\n",
    "                INSERT INTO llama3_pair(reviewer_name, gamea_name, gameb_name, model, prefered_game)\n",
    "                VALUES (%s, %s, %s, %s, %s)\n",
    "            \"\"\"\n",
    "\n",
    "for i in range(9769, 10000, 1):\n",
    "    user = users[i]\n",
    "    SQL = f\"\"\"\n",
    "        SELECT game_name, review_text\n",
    "        FROM game_reviews\n",
    "        WHERE reviewer_name = '{user}'\n",
    "    \"\"\"\n",
    "    queryer.execute(SQL)\n",
    "    reviews = queryer.fetchall()\n",
    "\n",
    "    processing_queue = []\n",
    "    for j in range(len(reviews)-1):\n",
    "        for k in range(j+1, len(reviews)):\n",
    "            processing_queue.append((reviews[j], reviews[k]))\n",
    "    \n",
    "    queue = prompt_generation(processing_queue)\n",
    "    \n",
    "    for j in range(0, len(processing_queue), 8): # batch size of 8        \n",
    "        msgs = queue[j:j+8]\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            msgs,\n",
    "            tokenize = True,\n",
    "            padding = True,\n",
    "            truncation = False,\n",
    "            add_generation_prompt = True, # Must add for generation\n",
    "            return_tensors = \"pt\",\n",
    "        )\n",
    "\n",
    "        \n",
    "        if(inputs.size(dim=1) > 1020): # if this batch contains very long review text, reduce the batch size to 2\n",
    "            for jj in range(0, len(msgs), 2):\n",
    "                msgs = queue[j+jj:j+jj+2]\n",
    "                inputs = tokenizer.apply_chat_template(\n",
    "                    msgs,\n",
    "                    tokenize = True,\n",
    "                    padding = True,\n",
    "                    truncation = False,\n",
    "                    add_generation_prompt = True, # Must add for generation\n",
    "                    return_tensors = \"pt\",\n",
    "                ).to('cuda')\n",
    "                if(inputs.size(dim=1) >= 4090): # exceed the limit of llama3\n",
    "                    continue\n",
    "                    \n",
    "                attention_mask = (inputs != tokenizer.pad_token_id).int()\n",
    "    \n",
    "                outputs = model.generate(input_ids = inputs, max_new_tokens = 4096, attention_mask = attention_mask, use_cache = True)\n",
    "                gen_idx = len(inputs[0])\n",
    "                result = tokenizer.batch_decode(outputs[:, gen_idx:], skip_special_tokens = True)\n",
    "\n",
    "                for k in range(len(result)):\n",
    "                    if normalize_title(result[k]) == normalize_title(processing_queue[j+jj+k][0][0]):\n",
    "                        prefered_game = processing_queue[j+jj+k][0][0]\n",
    "                    elif normalize_title(result[k]) == normalize_title(processing_queue[j+jj+k][1][0]):\n",
    "                        prefered_game = processing_queue[j+jj+k][1][0]\n",
    "                    else:\n",
    "                        prefered_game = \"CannotJudge\"\n",
    "        \n",
    "                    saver.execute(Insert_SQL, (user, processing_queue[j+jj+k][0][0], processing_queue[j+jj+k][1][0], \"unsloth_e329\", prefered_game))\n",
    "                conn.commit()\n",
    "            continue\n",
    "\n",
    "        inputs.to('cuda')\n",
    "        \n",
    "        attention_mask = (inputs != tokenizer.pad_token_id).int()\n",
    "\n",
    "        outputs = model.generate(input_ids = inputs, max_new_tokens = 4096, attention_mask = attention_mask, use_cache = True)\n",
    "        gen_idx = len(inputs[0])\n",
    "        result = tokenizer.batch_decode(outputs[:, gen_idx:], skip_special_tokens = True)\n",
    "\n",
    "        for k in range(len(result)):\n",
    "            if normalize_title(result[k]) == normalize_title(processing_queue[j+k][0][0]):\n",
    "                prefered_game = processing_queue[j+k][0][0]\n",
    "            elif normalize_title(result[k]) == normalize_title(processing_queue[j+k][1][0]):\n",
    "                prefered_game = processing_queue[j+k][1][0]\n",
    "            else:\n",
    "                prefered_game = \"CannotJudge\"\n",
    "\n",
    "            saver.execute(Insert_SQL, (user, processing_queue[j+k][0][0], processing_queue[j+k][1][0], \"unsloth_e329\", prefered_game))\n",
    "        conn.commit()\n",
    "\n",
    "    if i%25 == 0:\n",
    "        print(f\"processed {i} users, {i}th user is {user}\")\n",
    "        torch.cuda.empty_cache() # clear cache every 25 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43ee7b-0390-4b15-b56c-0bb5e8eaa298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
