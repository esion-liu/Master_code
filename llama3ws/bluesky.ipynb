{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a480c4e-3ca4-4ef8-b3a7-6efd5b30039b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth: Fast Llama patching release 2024.7\n",
      "   \\\\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.988 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0+cu121. CUDA = 8.9. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.7 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "import torch\n",
    "import json\n",
    "import os.path as path\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"ll3_fitting/epoch_329\",\n",
    "    max_seq_length = 4096,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "257d6da2-85b2-4e4a-a41a-3ec5f48245b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3\"\n",
    ")\n",
    "tokenizer.pad_token = '<|reserved_special_token_250|>'\n",
    "tokenizer.pad_token_id = 128255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05be17c1-3258-4fae-8f44-9ea314401eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample mesasge batch\n",
    "messages_batch = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Your role is to judge which product user prefer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"For Product A I give a review: 'This product is good', while for product B I give a revie: 'This product is very good'.\"}\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Your role is to judge which product user prefer.\"},\n",
    "        {\"role\": \"user\", \"content\": \"For Product A I give a review: 'This product is good', while for product B I give a revie: 'I have no time to sleep and eat after purchasing this product!'.\"}\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba382dd-213d-45c6-a11c-ffdaca9e49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages_batch,\n",
    "    tokenize = True,\n",
    "    padding = True,\n",
    "    truncation = False,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = 'pt',).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e2190df-1103-4665-ba07-3f9a30f7b14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128255, 128255, 128255, 128255, 128255, 128255, 128255, 128255, 128000,\n",
       "         128006,   9125, 128007,    271,   7927,   3560,    374,    311,  11913,\n",
       "            902,   2027,   1217,  10932,     13, 128009, 128006,    882, 128007,\n",
       "            271,   2520,   5761,    362,    358,   3041,    264,   3477,     25,\n",
       "            364,   2028,   2027,    374,   1695,    518,   1418,    369,   2027,\n",
       "            426,    358,   3041,    264,   5891,    648,     25,    364,   2028,\n",
       "           2027,    374,   1633,   1695,   4527, 128009, 128006,  78191, 128007,\n",
       "            271],\n",
       "        [128000, 128006,   9125, 128007,    271,   7927,   3560,    374,    311,\n",
       "          11913,    902,   2027,   1217,  10932,     13, 128009, 128006,    882,\n",
       "         128007,    271,   2520,   5761,    362,    358,   3041,    264,   3477,\n",
       "             25,    364,   2028,   2027,    374,   1695,    518,   1418,    369,\n",
       "           2027,    426,    358,   3041,    264,   5891,    648,     25,    364,\n",
       "             40,    617,    912,    892,    311,   6212,    323,   8343,   1306,\n",
       "          23395,    420,   2027,      0,   4527, 128009, 128006,  78191, 128007,\n",
       "            271]], device='cuda:0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5093881a-b3fc-426e-8870-4fd8978e1e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0',\n",
       "       dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (inputs != tokenizer.pad_token_id).int()\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a94092-1fd0-4fd6-9f67-dd4a8306a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(input_ids = inputs, attention_mask = attention_mask, max_new_tokens = 256, use_cache = True)\n",
    "gen_idx = len(inputs[0])\n",
    "result = tokenizer.batch_decode(outputs[:], skip_special_tokens = True)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f154ed1-80d1-4416-a979-e23ce564c8de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
